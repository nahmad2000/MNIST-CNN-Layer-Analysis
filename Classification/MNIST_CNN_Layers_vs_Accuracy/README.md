# MNIST CNN Layer Analysis: Accuracy vs. Parameters

This Jupyter Notebook explores the effect of adding convolutional layers on the performance and complexity of a neural network for classifying handwritten digits from the MNIST dataset. This project was completed as part of an assignment (Question 6) for a Digital Image Processing course.

## Overview

The goal is to understand how model architecture, specifically the addition of Convolutional Neural Network (CNN) layers (Conv2D and MaxPooling2D), impacts:
1.  **Test Accuracy:** How well the model classifies unseen digits.
2.  **Model Complexity:** Measured by the total number of trainable parameters.

The analysis starts with a simple baseline model using only fully connected (Dense) layers and progressively adds convolutional and pooling layers, observing the changes in performance and parameter count.

## Methodology

1.  **Data Loading & Preprocessing:**
    * The standard MNIST dataset (60k training, 10k testing images of 28x28 pixels) is loaded using `tensorflow.keras.datasets.mnist`.
    * Images are reshaped to include a channel dimension `(28, 28, 1)`.
    * Pixel values are normalized to the range [0, 1].
    * Labels are one-hot encoded.

2.  **Models Built:**
    * **Baseline_1FC:** A simple model with one hidden Dense layer (128 neurons) after flattening the input image.
    * **Model_1Conv:** Introduces a Conv2D layer (32 filters, 3x3 kernel) and a MaxPooling2D layer (2x2) before flattening and passing to the Dense layers.
    * **Model_2Conv:** Adds a second Conv2D layer (64 filters, 3x3 kernel) and MaxPooling2D layer before flattening and the Dense layers.

3.  **Training & Evaluation:**
    * Each model is compiled using the Adam optimizer and categorical cross-entropy loss.
    * Models are trained for 5 epochs with a batch size of 32.
    * Test accuracy and the total number of trainable parameters (`model.count_params()`) are recorded for each model.

4.  **Visualization:**
    * A scatter plot is generated showing Test Accuracy vs. Number of Parameters (using a log scale for parameters) to visualize the relationship between complexity and performance.

## Results Summary

The notebook demonstrates the following trends (specific values are generated upon running the notebook):

* Adding the first Conv/Pool block (`Model_1Conv`) significantly increased accuracy compared to the `Baseline_1FC`.
* Adding the second Conv/Pool block (`Model_2Conv`) further improved accuracy.
* Interestingly, `Model_1Conv` had the highest number of parameters due to the large Dense layer following the flatten operation on its feature maps. `Model_2Conv`, despite being deeper, had fewer parameters than `Model_1Conv` because the second pooling operation reduced the feature map size considerably before the dense layer connection.

*(The plot generated by the notebook visually confirms these relationships.)*

## Analysis & Conclusion

* **Initial Accuracy Increase:** Convolutional layers are highly effective for image tasks because they learn spatial hierarchies of features (edges, patterns, etc.), leading to better performance than dense layers operating on flattened pixels.
* **Potential Accuracy Decrease (Overfitting):** While not definitively observed within just 5 epochs and 3 models, the assignment prompts consideration of why accuracy might eventually decrease as complexity increases further. The primary reason is **overfitting**. As models become too complex (too many parameters), they risk memorizing the training data noise instead of learning generalizable patterns, leading to poor performance on unseen test data.
* **Parameter Efficiency:** While CNNs *can* be parameter-efficient, the overall count is heavily influenced by the size of dense layers connected after the convolutional blocks. Deeper pooling helps reduce the input size to these dense layers, often leading to better parameter efficiency in deeper CNNs compared to shallower ones with large dense heads (as seen comparing `Model_2Conv` to `Model_1Conv`).

## How to Run

1.  Ensure you have the required dependencies installed (see below).
2.  Open the `.ipynb` file in Jupyter Notebook, JupyterLab, Google Colab, or a compatible environment (like VS Code with Python/Jupyter extensions).
3.  Run the cells sequentially from top to bottom.

## Dependencies

* TensorFlow / Keras (`tensorflow`)
* NumPy (`numpy`)
* Matplotlib (`matplotlib`)